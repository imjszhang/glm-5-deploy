# GLM-5 系统架构说明

> 文档创建时间：2026-02-13

---

## 一、系统概览

GLM-5 采用**单层部署架构**：模型推理服务直接运行在宿主机上，通过 llama-server 提供 OpenAI 兼容 API。无需 Docker、前端、后端或 LiveKit。

```
┌─────────────────────────────────────────────────────────────────┐
│                    用户 / OpenAI 客户端                            │
│               http://localhost:8001/v1                           │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────┐
│                      宿主机推理服务                                │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  llama-server (C++)                                      │   │
│  │  端口: 8001                                              │   │
│  │  功能: LLM 推理, OpenAI 兼容 API                         │   │
│  │  模型: GLM-5-UD-IQ2_XXS (分片 GGUF, ~241GB)              │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  GGUF 模型文件 (models/GLM-5-GGUF/UD-IQ2_XXS/)           │   │
│  │  ├── GLM-5-UD-IQ2_XXS-00001-of-00006.gguf                │   │
│  │  ├── ...                                                 │   │
│  │  └── GLM-5-UD-IQ2_XXS-00006-of-00006.gguf                │   │
│  └──────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
```

---

## 二、服务组件

| 组件 | 端口 | 说明 |
|------|------|------|
| **llama-server** | 8001 | C++ 推理引擎，加载 GGUF 模型，提供 OpenAI 兼容 HTTP API |

---

## 三、数据流

```
OpenAI Client / curl → llama-server(:8001) → GGUF 模型
                              ↓
OpenAI Client / curl ← JSON 响应 ←
```

---

## 四、模型文件

| 量化版本 | 路径 | 大小 |
|----------|------|------|
| UD-IQ2_XXS (2-bit) | `models/GLM-5-GGUF/UD-IQ2_XXS/*.gguf` | ~241GB |
| UD-TQ1_0 (1-bit) | `models/GLM-5-GGUF/UD-TQ1_0/*.gguf` | ~176GB |

---

## 五、关键配置文件

| 文件 | 说明 |
|------|------|
| `deploy.sh` | 一键部署脚本 |
| `download_models.sh` | 模型下载脚本 |
| `DEPLOY.md` | 部署文档 |

---

## 六、常用运维命令

```bash
# 启动服务
./deploy.sh --cpp-dir /path/to/llama.cpp --model-dir /path/to/models/GLM-5-GGUF/UD-IQ2_XXS

# 健康检查
curl http://localhost:8001/health

# 停止服务
pkill -f llama-server
```
